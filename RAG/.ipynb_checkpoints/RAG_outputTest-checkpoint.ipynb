{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f9041d-7a9b-4d60-89d7-b5d7c83ed3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/RAG'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5414ffbf-bf32-4f53-ad20-ce22b75f3d75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# === RAG: 基础配置 ===\n",
    "import os, json, math, pickle, gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/root/customs_tokenizers/\")  \n",
    "\n",
    "BASE_MODEL_DIR    = \"/root/bert-base-chinese\"                       # 本地 BERT\n",
    "LORA_WEIGHT       = \"/root/models/model_best_stroke_lora.pth\"       # LoRA 权重\n",
    "MERGED_ENCODER    = \"/root/models/model_merged_stroke\"             \n",
    "QWEN_DIR          = \"/root/autodl-tmp/Qwen2.5-1.5B\"                 # 本地 Qwen 1.5B\n",
    "CORPUS_JSON       = \"RAG_data_50k.json\"                      # 语料\n",
    "RAG_DIR           = Path(\"/root/RAG\")\n",
    "RAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_PATH        = RAG_DIR / \"faiss.index\"\n",
    "CHUNKS_META_PATH  = RAG_DIR / \"chunks.pkl\"\n",
    "from pinyin_tokenizer import PinyinTokenizer\n",
    "from stroke_tokenizer import StrokeTokenizer\n",
    "\n",
    "# RAG 参数\n",
    "CHUNK_SIZE   = 250    \n",
    "CHUNK_OVERLAP = 40    \n",
    "TOP_K        = 5    \n",
    "BATCH_SIZE   = 64      \n",
    "MAX_LEN      = 128    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af3d52ed-cab2-4477-ac42-2203caee80b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder loaded (LoRA). Hidden: 768\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def load_encoder():\n",
    "    # 优先加载合并后的编码器\n",
    "    if Path(MERGED_ENCODER).exists():\n",
    "        tok = AutoTokenizer.from_pretrained(MERGED_ENCODER, local_files_only=True)\n",
    "        enc = AutoModel.from_pretrained(MERGED_ENCODER, local_files_only=True).to(DEVICE).eval()\n",
    "        print(\"Encoder loaded (merged). Hidden:\", enc.config.hidden_size)\n",
    "        return tok, enc\n",
    "\n",
    "    # 否则按 LoRA 方式加载\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, local_files_only=True)\n",
    "    base = AutoModel.from_pretrained(BASE_MODEL_DIR, local_files_only=True)\n",
    "\n",
    "    # 与训练一致的 LoRA 配置\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=8, lora_alpha=16, target_modules=[\"query\",\"key\",\"value\"],\n",
    "        lora_dropout=0.1, bias=\"none\"\n",
    "    )\n",
    "    enc = get_peft_model(base, peft_cfg)\n",
    "\n",
    "    # 加载权重（修正键名前缀）\n",
    "    sd = torch.load(LORA_WEIGHT, map_location=\"cpu\")\n",
    "    new_sd = { (k.replace(\"base_model.model.\",\"base_model.\") if k.startswith(\"base_model.model.\") else k): v\n",
    "               for k,v in sd.items() }\n",
    "    try:\n",
    "        enc.load_state_dict(new_sd, strict=True)\n",
    "    except Exception:\n",
    "        enc.load_state_dict(new_sd, strict=False)\n",
    "\n",
    "    enc = enc.to(DEVICE).eval()\n",
    "    print(\"Encoder loaded (LoRA). Hidden:\", enc.base_model.config.hidden_size if hasattr(enc,\"base_model\") else enc.config.hidden_size)\n",
    "    return tok, enc\n",
    "\n",
    "enc_tok, enc_model = load_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c587b06-64b7-4e83-96fe-154e378572c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready.\n"
     ]
    }
   ],
   "source": [
    "def load_llm():\n",
    "    llm_tok = AutoTokenizer.from_pretrained(QWEN_DIR, trust_remote_code=True, local_files_only=True)\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        QWEN_DIR, trust_remote_code=True, local_files_only=True\n",
    "    ).to(DEVICE).eval()\n",
    "    return llm_tok, llm\n",
    "\n",
    "llm_tok, llm = load_llm()\n",
    "print(\"LLM ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9532b9cb-6529-4eb2-98a3-b700e364417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded docs: 45000\n",
      "Total chunks: 115214\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(json_path: str) -> List[Dict]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # 兼容 [{'query','document'}] 或 [{'question','answer'}]\n",
    "    out = []\n",
    "    for r in data:\n",
    "        doc = r.get(\"document\") or r.get(\"answer\") or \"\"\n",
    "        q   = r.get(\"query\") or r.get(\"question\") or \"\"\n",
    "        if doc:\n",
    "            out.append({\"query\": q, \"document\": doc})\n",
    "    return out\n",
    "\n",
    "from stroke_tokenizer import StrokeTokenizer\n",
    "tok_stroke = StrokeTokenizer()\n",
    "\n",
    "from pinyin_tokenizer import PinyinTokenizer\n",
    "\n",
    "tok_pinyin = PinyinTokenizer()\n",
    "\n",
    "def chunk_text(text: str, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"基于拼音 tokenizer 的 token 级分块\"\"\"\n",
    "    token_ids = tok_pinyin.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(token_ids):\n",
    "        end = start + size\n",
    "        sub_ids = token_ids[start:end]\n",
    "        # 把 token id 转回对应 token\n",
    "        sub_tokens = [tok_pinyin.id2token[i] for i in sub_ids if i in tok_pinyin.id2token]\n",
    "        chunk = \"\".join(sub_tokens)\n",
    "        chunks.append(chunk)\n",
    "        if end >= len(token_ids):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "corpus = load_corpus(CORPUS_JSON)\n",
    "print(\"Loaded docs:\", len(corpus))\n",
    "\n",
    "# 构建 chunk 元信息\n",
    "chunks = []\n",
    "for i, r in enumerate(corpus):\n",
    "    for c in chunk_text(r[\"document\"], CHUNK_SIZE, CHUNK_OVERLAP):\n",
    "        chunks.append({\"doc_id\": i, \"text\": c})\n",
    "print(\"Total chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76220dc3-fb01-48b8-8d30-7f4831e8780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded: 11001\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def encode_texts(texts: List[str]) -> torch.Tensor:\n",
    "    all_vecs = []\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+BATCH_SIZE]\n",
    "        enc = enc_tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        out = enc_model(**enc, output_hidden_states=True)\n",
    "        cls = out.hidden_states[-1][:, 0, :]  # [CLS]\n",
    "        all_vecs.append(cls.detach().float().cpu())\n",
    "    return torch.cat(all_vecs, dim=0) if all_vecs else torch.empty(0, dtype=torch.float32)\n",
    "\n",
    "def build_or_load_index(chunks, index_path=INDEX_PATH, meta_path=CHUNKS_META_PATH):\n",
    "    if index_path.exists() and meta_path.exists():\n",
    "        index = faiss.read_index(str(index_path))\n",
    "        with open(meta_path, \"rb\") as f:\n",
    "            meta = pickle.load(f)\n",
    "        print(\"Index loaded:\", index.ntotal)\n",
    "        return index, meta\n",
    "\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    vecs = encode_texts(texts).numpy().astype(\"float32\")  # [N, D]\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    # 向量先归一化，可用余弦相似度\n",
    "    faiss.normalize_L2(vecs)\n",
    "    index.add(vecs)\n",
    "\n",
    "    faiss.write_index(index, str(index_path))\n",
    "    with open(meta_path, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    print(\"Index built:\", index.ntotal, \"dim:\", dim)\n",
    "    return index, chunks\n",
    "\n",
    "index, chunks_meta = build_or_load_index(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a00d33e-cb0d-435f-af88-f9656893be76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(query: str, top_k=TOP_K):\n",
    "    qv = encode_texts([query]).numpy().astype(\"float32\")\n",
    "    faiss.normalize_L2(qv)\n",
    "    D, I = index.search(qv, top_k)   # 余弦相似度\n",
    "    I = I[0].tolist()\n",
    "    D = D[0].tolist()\n",
    "    results = []\n",
    "    for idx, score in zip(I, D):\n",
    "        meta = chunks_meta[idx]\n",
    "        results.append({\"text\": meta[\"text\"], \"doc_id\": meta[\"doc_id\"], \"score\": float(score)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6018a3d-b7c3-4539-95fe-46defc35477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer as HFTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# 本地 Qwen LLM\n",
    "qwen_tok = HFTokenizer.from_pretrained(QWEN_DIR, local_files_only=True, trust_remote_code=True)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(QWEN_DIR, local_files_only=True, trust_remote_code=True).to(DEVICE).eval()\n",
    "\n",
    "# transformers 的 pipeline（别和上面的“自定义分词器”混用，这是给 LLM 生成用的）\n",
    "qwen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=qwen_model,\n",
    "    tokenizer=qwen_tok,\n",
    "    device=0 if DEVICE == \"cuda\" else -1,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eec38cbb-89ac-408f-b20b-e0e1cf5efaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_908/1653740109.py:6: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  llm_langchain = HuggingFacePipeline(pipeline=qwen_pipe)\n",
      "/tmp/ipykernel_908/1653740109.py:19: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  rag_chain = LLMChain(prompt=prompt_template, llm=llm_langchain)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 把 transformers 的 pipeline 包装成 LangChain 的 LLM\n",
    "llm_langchain = HuggingFacePipeline(pipeline=qwen_pipe)\n",
    "\n",
    "# Prompt 模板\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"你是一名医疗健康助手。请根据以下资料回答问题：\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"问题：{question}\\n回答：\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# LLMChain\n",
    "rag_chain = LLMChain(prompt=prompt_template, llm=llm_langchain)\n",
    "\n",
    "# 生成函数（替换你原来的 generate_answer）\n",
    "# === 替换 generate_answer 函数（兼容 ctx 为字符串或字典）===\n",
    "def generate_answer(query: str, top_k=TOP_K):\n",
    "    ctxs = retrieve(query, top_k=top_k)           # ctxs: 可能是 dict 或 str\n",
    "\n",
    "    # 统一拿到字符串\n",
    "    ctx_texts = [c[\"text\"] if isinstance(c, dict) else c for c in ctxs]\n",
    "\n",
    "    # 拼接给 LLM 的上下文\n",
    "    context_text = \"\\n\".join([f\"- {t}\" for t in ctx_texts])\n",
    "\n",
    "    # LangChain 调用\n",
    "    raw_output = rag_chain.run({\"context\": context_text, \"question\": query})\n",
    "\n",
    "    # 轻度清理\n",
    "    answer = raw_output.split(\"回答：\")[-1].strip()\n",
    "    answer = answer.replace(\"你是一名医疗健康助手。\", \"\").strip()\n",
    "\n",
    "    # 返回答案 + 纯文本 ctx 列表\n",
    "    return answer, ctx_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43229a3c-96c1-4192-9112-63bf89d60e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "/tmp/ipykernel_908/1653740109.py:33: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  raw_output = rag_chain.run({\"context\": context_text, \"question\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答案： 根据你的描述属于\n",
      "检索到的文档： ['你好，孩子缺钙,要补钙和鱼肝油', '你好，有糖尿病的话，饮食上需要注意不能吃含糖食物，注意控制能量摄入水平。可以常吃蔬菜或低糖含量的水果', '你好属于慢性感染引起的牙龈炎，需要停止哺乳，用吸奶器吸奶，因对因而有影响，增加营养补充维生素微量元素', '根据你的描述属于正常情况有关，一般是可以不用治疗的，需要正确对待增加营养补充维生素微量元素，易消化易', '根据你的描述症状有可能避孕失败，需要上医院复查，建议增加营养补充维生素补充蛋白质，易消化易吸收饮食，']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 76.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "问题: 糖尿病患者早餐可以吃什么？\n",
      "答案: 糖尿病患者早餐可以吃一些低糖、高纤维的食物，如燕麦粥、全麦面包、鸡蛋、牛奶、水果等。同时，建议糖尿病患者在饮食上要控制总热量，避免过量摄入糖分和脂肪，以维持血糖水平的稳定。\n",
      "Top-3 检索片段: ['你好，孩子缺钙,要补钙和鱼肝油', '你好属于慢性感染引起的牙龈炎，需要停止哺乳，用吸奶器吸奶，因对因而有影响，增加营养补充维生素微量元素', '尿路感染引起的症状建议及时的多喝水注意休息均衡营养多吃新鲜蔬菜水果口服抗生素和输液抗生素的方法治疗口']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 75.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "问题: 胃炎反复发作应该怎么调理？\n",
      "答案: 胃炎反复发作，建议您采取以下措施进行调理：\n",
      "\n",
      "1. **饮食调整**：避免辛辣、油腻、过热或过冷的食物，减少咖啡因和酒精的摄入。选择易消化、营养丰富的食物，如粥、面条、蒸蛋等。\n",
      "\n",
      "2. **规律作息**：保持充足的睡眠，避免熬夜，有助于身体恢复和免疫系统的正常运作。\n",
      "\n",
      "3. **适量运动**：适当的体育活动可以增强体质，改善消化功能，但应避免剧烈运动。\n",
      "\n",
      "4. **心理调适**：保持良好的心态，避免过度紧张和焦虑，因为情绪波动也可能影响胃部健康。\n",
      "\n",
      "5. **药物治疗**：根据医生的指导使用抗酸药、胃黏膜保护剂等药物，以减轻症状和促进愈合。\n",
      "\n",
      "6. **定期复查**：定期到医院进行胃镜检查和其他相关检查，以便及时了解病情变化并调整治疗方案。\n",
      "\n",
      "7. **中医调理**：可以考虑采用中药调理，如服用具有健脾和胃作用的中药方剂，但需在专业中医师的指导下进行。\n",
      "\n",
      "请注意，以上建议仅供参考，具体治疗方案应由专业医生根据您的具体情况制定。如果症状持续或加重，请及时就医。\n",
      "Top-3 检索片段: ['请问我女儿两岁半了，现在发烧38.2度，可是不吃药，怎么办？38,2度是不是烧的很严重？怎么治疗？', '你好，孩子缺钙,要补钙和鱼肝油', '尿路感染引起的症状建议及时的多喝水注意休息均衡营养多吃新鲜蔬菜水果口服抗生素和输液抗生素的方法治疗口']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00, 62.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "问题: 孕期贫血需要补充哪些营养？\n",
      "答案: 孕期贫血需要补充的营养包括：正确对待增加营养补充维生素微量元素，易消化易吸收饮食。\n",
      "Top-3 检索片段: ['你好，孩子缺钙,要补钙和鱼肝油', '你好属于慢性感染引起的牙龈炎，需要停止哺乳，用吸奶器吸奶，因对因而有影响，增加营养补充维生素微量元素', '你好，钙片奶粉可以同时吃的，注意孕期保健和日常护理，适当补充微量元素为好的，定期孕检为好的，注意健康']\n"
     ]
    }
   ],
   "source": [
    "# 测试：单条\n",
    "ans, ctxs = generate_answer(\"糖尿病人早餐可以吃什么？\")\n",
    "print(\"答案：\", ans)\n",
    "print(\"检索到的文档：\", [c[:50] for c in ctxs])  # 这里改成直接切片字符串\n",
    "\n",
    "# 批量测试\n",
    "test_queries = [\n",
    "    \"糖尿病患者早餐可以吃什么？\",\n",
    "    \"胃炎反复发作应该怎么调理？\",\n",
    "    \"孕期贫血需要补充哪些营养？\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    ans, ctxs = generate_answer(q)\n",
    "    print(f\"\\n问题: {q}\")\n",
    "    print(\"答案:\", ans)\n",
    "    print(\"Top-3 检索片段:\", [c[:50] for c in ctxs[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1034065d-ebc7-4a57-b9d3-09d68c4ddb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook RAG_outputTest.ipynb to html\n",
      "[NbConvertApp] Writing 630095 bytes to RAG_outputTest.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html RAG_outputTest.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6c191-4e58-4381-818f-60ee53d8d978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc5dc4-f753-4be3-8358-e50aaa809129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf4aba-96b9-43da-bcd3-c0965f8231cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
