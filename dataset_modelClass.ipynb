{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae0778d-a8c0-4e51-a3a1-f1722c1335ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境信息\n",
      "PyTorch: 2.0.0+cu118\n",
      "设备: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "VRAM: 23.5 GB\n",
      "\n",
      "环境配置完成！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pypinyin import lazy_pinyin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "print(\"环境信息\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n环境配置完成！\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf31c8c3-0dac-4944-9324-53a986863b41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "定义训练组件...\n",
      "\n",
      "测试模型加载...\n",
      "Tokenizer 加载成功，词汇表大小: 21128\n",
      "字符级测试: '糖尿病的症状' -> ['糖', '尿', '病', '的', '症', '状']\n",
      "拼音级测试: 'tang niao bing de zheng zhuang' -> ['tan', '##g', 'ni', '##ao', 'bing', 'de', 'zh', '##eng', 'zh', '##uan', '##g']\n",
      "\n",
      "所有组件定义完成！\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 定义数据集和模型类\n",
    "print(\"定义训练组件...\")\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ============ 使用本地模型 ============\n",
    "MODEL_NAME = '/root/bert-base-chinese'\n",
    "# ====================================\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"配对数据集 - 支持字符级和拼音级\"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=128, use_pinyin=False):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.use_pinyin = use_pinyin\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # 根据 use_pinyin 选择数据源\n",
    "        if self.use_pinyin:\n",
    "            # 拼音级：使用转换后的拼音文本\n",
    "            query_text = item['query']  # 在 pinyin_data 中，这已经是拼音了\n",
    "            doc_text = item['document']\n",
    "        else:\n",
    "            # 字符级：使用原始中文\n",
    "            query_text = item['query']\n",
    "            doc_text = item['document']\n",
    "        \n",
    "        # Tokenize\n",
    "        query_enc = self.tokenizer(\n",
    "            query_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        doc_enc = self.tokenizer(\n",
    "            doc_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_enc['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_enc['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': doc_enc['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': doc_enc['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    \"\"\"基于 BERT 的 Sentence Embedding 模型\"\"\"\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super().__init__()\n",
    "        print(f\"初始化模型: {model_name}\")\n",
    "        self.encoder = BertModel.from_pretrained(model_name)\n",
    "        print(f\"✓ 模型加载完成\")\n",
    "        \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        \"\"\"平均池化\"\"\"\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(\n",
    "            token_embeddings.size()\n",
    "        ).float()\n",
    "        sum_embeddings = torch.sum(\n",
    "            token_embeddings * input_mask_expanded, 1\n",
    "        )\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        embeddings = self.mean_pooling(\n",
    "            outputs.last_hidden_state,\n",
    "            attention_mask\n",
    "        )\n",
    "        # L2 归一化\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(query_emb, doc_emb):\n",
    "    \"\"\"余弦相似度损失\"\"\"\n",
    "    similarity = F.cosine_similarity(query_emb, doc_emb, dim=1)\n",
    "    loss = 1 - similarity.mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# 测试加载\n",
    "print(\"\\n测试模型加载...\")\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer 加载成功，词汇表大小: {len(tokenizer)}\")\n",
    "    \n",
    "    # 测试字符级分词\n",
    "    test_text = \"糖尿病的症状\"\n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    print(f\"字符级测试: '{test_text}' -> {tokens}\")\n",
    "    \n",
    "    # 测试拼音级分词\n",
    "    from pypinyin import lazy_pinyin\n",
    "    pinyin_text = ' '.join(lazy_pinyin(test_text))\n",
    "    pinyin_tokens = tokenizer.tokenize(pinyin_text)\n",
    "    print(f\"拼音级测试: '{pinyin_text}' -> {pinyin_tokens}\")\n",
    "    \n",
    "    print(\"\\n所有组件定义完成！\")\n",
    "except Exception as e:\n",
    "    print(f\"加载失败: {e}\")\n",
    "    print(\"请检查模型路径是否正确\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35dfa52a-19b5-45ec-b206-179a1684cfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证模型加载...\n",
      "检查必需文件:\n",
      "  ✓ config.json\n",
      "  ✓ pytorch_model.bin\n",
      "  ✓ tokenizer_config.json\n",
      "  ✓ vocab.txt\n",
      "\n",
      "加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 词汇表大小: 21128\n",
      "模型参数量: 102.3M\n",
      "\n",
      "测试推理...\n",
      "输入 shape: torch.Size([1, 13])\n",
      "输出 shape: torch.Size([1, 13, 768])\n",
      "分词结果: ['糖', '尿', '病', '患', '者', '应', '该', '注', '意', '饮', '食']\n",
      "\n",
      "模型加载和推理测试成功！\n"
     ]
    }
   ],
   "source": [
    "# 验证本地模型\n",
    "print(\"验证模型加载...\")\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "MODEL_PATH = '/root/bert-base-chinese'\n",
    "\n",
    "# 1. 检查文件是否存在\n",
    "import os\n",
    "required_files = [\n",
    "    'config.json',\n",
    "    'pytorch_model.bin',\n",
    "    'tokenizer_config.json',\n",
    "    'vocab.txt'\n",
    "]\n",
    "\n",
    "print(\"检查必需文件:\")\n",
    "for file in required_files:\n",
    "    path = os.path.join(MODEL_PATH, file)\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"✓\" if exists else \"\"\n",
    "    print(f\"  {status} {file}\")\n",
    "\n",
    "# 2. 加载并测试\n",
    "print(\"\\n加载模型...\")\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = BertModel.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "    print(f\"Tokenizer 词汇表大小: {len(tokenizer)}\")\n",
    "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\n",
    "    \n",
    "    # 3. 测试推理\n",
    "    print(\"\\n测试推理...\")\n",
    "    text = \"糖尿病患者应该注意饮食\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    print(f\"输入 shape: {inputs['input_ids'].shape}\")\n",
    "    print(f\"输出 shape: {outputs.last_hidden_state.shape}\")\n",
    "    print(f\"分词结果: {tokenizer.tokenize(text)}\")\n",
    "    \n",
    "    print(\"\\n模型加载和推理测试成功！\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524aa4d8-4412-48e8-a5df-1a1628d63610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练三个版本的模型...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'char_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/root/bert-base-chinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. 字符级\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m char_dataset \u001b[38;5;241m=\u001b[39m PairDataset(\u001b[43mchar_data\u001b[49m, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     11\u001b[0m char_loader \u001b[38;5;241m=\u001b[39m DataLoader(char_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m char_model \u001b[38;5;241m=\u001b[39m EmbeddingModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/root/bert-base-chinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 7: 训练三个版本的模型\n",
    "print(\"训练三个版本的模型...\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('/root/bert-base-chinese')\n",
    "\n",
    "# 1. 字符级\n",
    "char_dataset = PairDataset(char_data, tokenizer, max_length=128)\n",
    "char_loader = DataLoader(char_dataset, batch_size=16, shuffle=True)\n",
    "char_model = EmbeddingModel('/root/bert-base-chinese')\n",
    "char_model = train_model(char_model, char_loader, epochs=3, model_name=\"字符级\")\n",
    "torch.save(char_model.state_dict(), 'char_model.pt')\n",
    "\n",
    "# 2. 拼音级\n",
    "pinyin_dataset = PairDataset(pinyin_data, tokenizer, max_length=128)\n",
    "pinyin_loader = DataLoader(pinyin_dataset, batch_size=16, shuffle=True)\n",
    "pinyin_model = EmbeddingModel('/root/bert-base-chinese')\n",
    "pinyin_model = train_model(pinyin_model, pinyin_loader, epochs=3, model_name=\"拼音级\")\n",
    "torch.save(pinyin_model.state_dict(), 'pinyin_model.pt')\n",
    "\n",
    "# 3. 笔画级（StrokeNet 风格）\n",
    "stroke_dataset = PairDataset(stroke_data, tokenizer, max_length=128)\n",
    "stroke_loader = DataLoader(stroke_dataset, batch_size=16, shuffle=True)\n",
    "stroke_model = EmbeddingModel('/root/bert-base-chinese')\n",
    "stroke_model = train_model(stroke_model, stroke_loader, epochs=3, model_name=\"笔画级\")\n",
    "torch.save(stroke_model.state_dict(), 'stroke_model.pt')\n",
    "\n",
    "print(\"\\n✓ 三个模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f46f5-cf2a-4e5f-9ed6-67ed2452cd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
