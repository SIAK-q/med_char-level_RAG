{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e7f20a-11c7-400a-a41e-7f5b0f7bf07b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境信息\n",
      "PyTorch: 2.0.0+cu118\n",
      "设备: cpu\n",
      "\n",
      "环境配置完成！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: 环境初始化\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pypinyin import lazy_pinyin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "print(\"环境信息\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n环境配置完成！\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e86a87c-63c1-4624-a0a5-667733f445d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81641685-5312-4e71-8baa-8823b5e399fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%cd autodl-tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6e41c9-63bf-4547-983c-7a4e103d8917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oncology_肿瘤科: 75553 条\n",
      "IM_内科: 220606 条\n",
      "OAGD_妇产科: 183751 条\n",
      "原始总计: 479910 条\n",
      "随机采样: 80000 条\n",
      "最终使用: 80000 条\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 加载数据集（采样版）\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"Data_数据\"\n",
    "dept_files = {\n",
    "    \"Oncology_肿瘤科\": \"肿瘤科5-10000.csv\",\n",
    "    \"IM_内科\": \"内科5000-33000.csv\",\n",
    "    \"OAGD_妇产科\": \"妇产科6-28000.csv\",\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "for dept, filename in dept_files.items():\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(data_dir, dept, filename), \n",
    "        encoding='gb18030'  # 不限制nrows，加载全部\n",
    "    )\n",
    "    all_data.append(df[['ask', 'answer']])\n",
    "    print(f\"{dept}: {len(df)} 条\")\n",
    "\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "print(f\"原始总计: {len(combined_df)} 条\")\n",
    "\n",
    "# 随机采样80,000条\n",
    "SAMPLE_SIZE = 80000\n",
    "if len(combined_df) > SAMPLE_SIZE:\n",
    "    combined_df = combined_df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"随机采样: {SAMPLE_SIZE} 条\")\n",
    "\n",
    "medical_qa = DatasetDict({\n",
    "    'train': Dataset.from_pandas(combined_df, preserve_index=False)\n",
    "})\n",
    "\n",
    "print(f\"最终使用: {len(medical_qa['train'])} 条\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ebabf87-b17c-4aea-9870-c101ae94e11b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理数据...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'medical_qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m TOTAL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30000\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 提取数据\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mmedical_qa\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mask\u001b[39m\u001b[38;5;124m'\u001b[39m][:TOTAL]\n\u001b[1;32m     33\u001b[0m docs \u001b[38;5;241m=\u001b[39m medical_qa[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m][:TOTAL]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 划分\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'medical_qa' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 5: 数据准备（保存版）\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pypinyin import lazy_pinyin\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 检查是否有保存的数据\n",
    "if os.path.exists('data_30k.pkl'):\n",
    "    print(\"加载已保存数据...\")\n",
    "    with open('data_30k.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    char_data = data['char_data']\n",
    "    pinyin_data = data['pinyin_data']\n",
    "    train_q = data['train_q']\n",
    "    val_q = data['val_q']\n",
    "    test_q = data['test_q']\n",
    "    train_d = data['train_d']\n",
    "    val_d = data['val_d']\n",
    "    test_d = data['test_d']\n",
    "    \n",
    "    print(\"加载完成（耗时<1秒）\")\n",
    "\n",
    "else:\n",
    "    print(\"处理数据...\")\n",
    "    \n",
    "    # 数据规模\n",
    "    TOTAL = 30000\n",
    "    \n",
    "    # 提取数据\n",
    "    queries = medical_qa['train']['ask'][:TOTAL]\n",
    "    docs = medical_qa['train']['answer'][:TOTAL]\n",
    "    \n",
    "    # 划分\n",
    "    train_val_q, test_q, train_val_d, test_d = train_test_split(\n",
    "        queries, docs, test_size=0.1, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_q, val_q, train_d, val_d = train_test_split(\n",
    "        train_val_q, train_val_d, test_size=0.1111, random_state=42, shuffle=True  # 0.1111 = 3000/27000\n",
    "    )\n",
    "    \n",
    "    # 字符级\n",
    "    char_data = {\n",
    "        'train': [{'query': train_q[i], 'document': train_d[i]} for i in range(len(train_q))],\n",
    "        'val': [{'query': val_q[i], 'document': val_d[i]} for i in range(len(val_q))],\n",
    "        'test': [{'query': test_q[i], 'document': test_d[i]} for i in range(len(test_q))]\n",
    "    }\n",
    "    \n",
    "    # 拼音级\n",
    "    def to_pinyin(text):\n",
    "        return ' '.join(lazy_pinyin(text))\n",
    "    \n",
    "    print(\"\\n转换拼音（约2分钟）...\")\n",
    "    pinyin_data = {\n",
    "        'train': [\n",
    "            {'query': to_pinyin(train_q[i]), 'document': to_pinyin(train_d[i])} \n",
    "            for i in tqdm(range(len(train_q)), desc=\"训练集\")\n",
    "        ],\n",
    "        'val': [\n",
    "            {'query': to_pinyin(val_q[i]), 'document': to_pinyin(val_d[i])} \n",
    "            for i in tqdm(range(len(val_q)), desc=\"验证集\")\n",
    "        ],\n",
    "        'test': [\n",
    "            {'query': to_pinyin(test_q[i]), 'document': to_pinyin(test_d[i])} \n",
    "            for i in tqdm(range(len(test_q)), desc=\"测试集\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 保存\n",
    "    print(\"\\n保存数据...\")\n",
    "    with open('data_30k.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'char_data': char_data,\n",
    "            'pinyin_data': pinyin_data,\n",
    "            'train_q': train_q,\n",
    "            'val_q': val_q,\n",
    "            'test_q': test_q,\n",
    "            'train_d': train_d,\n",
    "            'val_d': val_d,\n",
    "            'test_d': test_d,\n",
    "        }, f)\n",
    "    \n",
    "    print(\"数据已保存至 data_30k.pkl\")\n",
    "\n",
    "# 统一输出\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"数据概览:\")\n",
    "print(f\"  字符级: 训练{len(char_data['train'])} / 验证{len(char_data['val'])} / 测试{len(char_data['test'])}\")\n",
    "print(f\"  拼音级: 训练{len(pinyin_data['train'])} / 验证{len(pinyin_data['val'])} / 测试{len(pinyin_data['test'])}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b08a03-b144-43d4-9789-8acc4b04da69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据质量检查:\n",
      "训练集查询: 24000 条\n",
      "训练集唯一查询: 20690 条\n",
      "训练集重复率: 13.79%\n",
      "\n",
      "验证集查询: 3000 条\n",
      "验证集唯一查询: 2617 条\n",
      "验证集重复率: 12.77%\n",
      "\n",
      "训练集和验证集重叠: 37 条\n",
      "重叠比例: 1.41%\n",
      "\n",
      "重叠样本示例:\n",
      "  1. 我得了子宫内膜异位症。...\n",
      "  2. 问题描述:...\n",
      "  3. 一年多前确诊的食道癌，确诊后直接做的手术，当时术后也做了化疗巩固了一下，在一个月前复查的时候说是复发...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: 检查数据重复和泄露\n",
    "print(\"数据质量检查:\")\n",
    "\n",
    "# 1. 检查训练集内部重复\n",
    "train_query_set = set(train_q)\n",
    "print(f\"训练集查询: {len(train_q)} 条\")\n",
    "print(f\"训练集唯一查询: {len(train_query_set)} 条\")\n",
    "print(f\"训练集重复率: {(len(train_q) - len(train_query_set)) / len(train_q) * 100:.2f}%\")\n",
    "\n",
    "# 2. 检查验证集内部重复\n",
    "val_query_set = set(val_q)\n",
    "print(f\"\\n验证集查询: {len(val_q)} 条\")\n",
    "print(f\"验证集唯一查询: {len(val_query_set)} 条\")\n",
    "print(f\"验证集重复率: {(len(val_q) - len(val_query_set)) / len(val_q) * 100:.2f}%\")\n",
    "\n",
    "# 3. 检查训练集和验证集重叠\n",
    "overlap = train_query_set.intersection(val_query_set)\n",
    "print(f\"\\n训练集和验证集重叠: {len(overlap)} 条\")\n",
    "print(f\"重叠比例: {len(overlap) / len(val_query_set) * 100:.2f}%\")\n",
    "\n",
    "# 4. 显示几个重叠的例子\n",
    "if len(overlap) > 0:\n",
    "    print(\"\\n重叠样本示例:\")\n",
    "    for i, q in enumerate(list(overlap)[:3]):\n",
    "        print(f\"  {i+1}. {q[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96a7836-e468-4563-b1e0-c1ee6daa54d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 组件定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 定义训练组件\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MODEL_NAME = '/root/bert-base-chinese'\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        def encode(text):\n",
    "            return self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        q_enc = encode(item['query'])\n",
    "        d_enc = encode(item['document'])\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': q_enc['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': q_enc['attention_mask'].squeeze(0),\n",
    "            'doc_input_ids': d_enc['input_ids'].squeeze(0),\n",
    "            'doc_attention_mask': d_enc['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, model_name=MODEL_NAME):\n",
    "        super().__init__()\n",
    "        self.encoder = BertModel.from_pretrained(model_name)\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "def cosine_loss(query_emb, doc_emb):\n",
    "    similarity = F.cosine_similarity(query_emb, doc_emb, dim=1)\n",
    "    # 防止数值不稳定\n",
    "    similarity = torch.clamp(similarity, -1.0, 1.0)\n",
    "    loss = 1 - similarity.mean()\n",
    "    # 添加小的epsilon防止为0\n",
    "    loss = loss + 1e-8\n",
    "    return loss\n",
    "\n",
    "print(\"✓ 组件定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71707dec-514c-4104-8f2f-cb3e38ba2b46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 字符级\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_char_dataset \u001b[38;5;241m=\u001b[39m PairDataset(\u001b[43mchar_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer)\n\u001b[1;32m      8\u001b[0m val_char_dataset \u001b[38;5;241m=\u001b[39m PairDataset(char_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer)\n\u001b[1;32m      9\u001b[0m test_char_dataset \u001b[38;5;241m=\u001b[39m PairDataset(char_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer)  \u001b[38;5;66;03m# 新增\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 7: 创建 DataLoader（添加测试集）\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 字符级\n",
    "train_char_dataset = PairDataset(char_data['train'], tokenizer)\n",
    "val_char_dataset = PairDataset(char_data['val'], tokenizer)\n",
    "test_char_dataset = PairDataset(char_data['test'], tokenizer)  # 新增\n",
    "\n",
    "train_char_loader = DataLoader(train_char_dataset, batch_size=32, shuffle=True)  # batch_size增大\n",
    "val_char_loader = DataLoader(val_char_dataset, batch_size=32, shuffle=False)\n",
    "test_char_loader = DataLoader(test_char_dataset, batch_size=32, shuffle=False)  # 新增\n",
    "\n",
    "# 拼音级\n",
    "train_pinyin_dataset = PairDataset(pinyin_data['train'], tokenizer)\n",
    "val_pinyin_dataset = PairDataset(pinyin_data['val'], tokenizer)\n",
    "test_pinyin_dataset = PairDataset(pinyin_data['test'], tokenizer)  # 新增\n",
    "\n",
    "train_pinyin_loader = DataLoader(train_pinyin_dataset, batch_size=32, shuffle=True)\n",
    "val_pinyin_loader = DataLoader(val_pinyin_dataset, batch_size=32, shuffle=False)\n",
    "test_pinyin_loader = DataLoader(test_pinyin_dataset, batch_size=32, shuffle=False)  # 新增\n",
    "\n",
    "print(f\"字符级: 训练{len(train_char_loader)} / 验证{len(val_char_loader)} / 测试{len(test_char_loader)} batches\")\n",
    "print(f\"拼音级: 训练{len(train_pinyin_loader)} / 验证{len(val_pinyin_loader)} / 测试{len(test_pinyin_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e6d59a-9d90-4a02-86f8-22afde934530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 训练函数（移除验证，最终版）\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, epochs=5, lr=2e-5, name=\"模型\"):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"[{name}] Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            query_ids = batch['query_input_ids'].to(device)\n",
    "            query_mask = batch['query_attention_mask'].to(device)\n",
    "            doc_ids = batch['doc_input_ids'].to(device)\n",
    "            doc_mask = batch['doc_attention_mask'].to(device)\n",
    "            \n",
    "            query_emb = model(query_ids, query_mask)\n",
    "            doc_emb = model(doc_ids, doc_mask)\n",
    "            loss = cosine_loss(query_emb, doc_emb)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        print(f\"\\n[{name}] Epoch {epoch+1}/{epochs}  训练loss: {avg_train_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f'{name}_epoch{epoch+1}.pt')\n",
    "        print()\n",
    "    \n",
    "    return model, train_losses\n",
    "\n",
    "print(\"训练函数定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7adead40-ec44-404e-a3cc-006c77222840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[字符级] Epoch 1/3: 100%|██████████| 750/750 [03:25<00:00,  3.64it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[字符级] Epoch 1/3  训练loss: 0.0038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[字符级] Epoch 2/3: 100%|██████████| 750/750 [03:25<00:00,  3.66it/s, loss=0.0007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[字符级] Epoch 2/3  训练loss: 0.0007\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[字符级] Epoch 3/3: 100%|██████████| 750/750 [03:25<00:00,  3.65it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[字符级] Epoch 3/3  训练loss: 0.0004\n",
      "\n",
      "字符级模型训练完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 训练字符级模型\n",
    "char_model = EmbeddingModel(MODEL_NAME)\n",
    "\n",
    "char_model, char_train_losses = train_model(  # 只返回train_losses\n",
    "    char_model,\n",
    "    train_char_loader,  \n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    name=\"字符级\"\n",
    ")\n",
    "\n",
    "print(\"字符级模型训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b88db8f8-b34e-475e-ace7-05a59a957ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[拼音级] Epoch 1/3: 100%|██████████| 750/750 [04:28<00:00,  2.79it/s, loss=0.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[拼音级] Epoch 1/3  训练loss: 0.0024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[拼音级] Epoch 2/3: 100%|██████████| 750/750 [04:28<00:00,  2.79it/s, loss=0.0027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[拼音级] Epoch 2/3  训练loss: 0.0005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[拼音级] Epoch 3/3: 100%|██████████| 750/750 [04:27<00:00,  2.80it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[拼音级] Epoch 3/3  训练loss: 0.0004\n",
      "\n",
      "拼音级模型训练完成\n"
     ]
    }
   ],
   "source": [
    "# 训练拼音级模型\n",
    "pinyin_model = EmbeddingModel(MODEL_NAME)\n",
    "\n",
    "pinyin_model, pinyin_train_losses = train_model(  # 只返回train_losses\n",
    "    pinyin_model,\n",
    "    train_pinyin_loader,  # 只传入train_loader\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    name=\"拼音级\"\n",
    ")\n",
    "\n",
    "print(\"拼音级模型训练完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99959e7b-8670-4d7a-9bb1-1a9022c8730c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: 定义评估函数\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pypinyin import lazy_pinyin\n",
    "\n",
    "def evaluate_retrieval(model, queries, documents, tokenizer, use_pinyin=False, k=10):\n",
    "    \"\"\"\n",
    "    评估检索性能\n",
    "    \n",
    "    Args:\n",
    "        model: 训练好的模型\n",
    "        queries: 查询列表\n",
    "        documents: 文档列表\n",
    "        tokenizer: tokenizer\n",
    "        use_pinyin: 是否使用拼音\n",
    "        k: Top-K\n",
    "    \n",
    "    Returns:\n",
    "        dict: 评估指标 {Recall@K, MRR}\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    # 如果是拼音，需要转换\n",
    "    if use_pinyin:\n",
    "        def to_pinyin(text):\n",
    "            return ' '.join(lazy_pinyin(text))\n",
    "        queries = [to_pinyin(q) for q in tqdm(queries, desc=\"  转换查询为拼音\")]\n",
    "        documents = [to_pinyin(d) for d in tqdm(documents, desc=\"  转换文档为拼音\")]\n",
    "    \n",
    "    # 编码查询\n",
    "    query_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for q in tqdm(queries, desc=\"  编码查询\"):\n",
    "            inputs = tokenizer(q, return_tensors='pt', max_length=128, \n",
    "                             truncation=True, padding='max_length')\n",
    "            q_emb = model(\n",
    "                inputs['input_ids'].to(device),\n",
    "                inputs['attention_mask'].to(device)\n",
    "            )\n",
    "            query_embeddings.append(q_emb.cpu().numpy())\n",
    "    \n",
    "    # 编码文档\n",
    "    doc_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(documents, desc=\"  编码文档\"):\n",
    "            inputs = tokenizer(d, return_tensors='pt', max_length=128,\n",
    "                             truncation=True, padding='max_length')\n",
    "            d_emb = model(\n",
    "                inputs['input_ids'].to(device),\n",
    "                inputs['attention_mask'].to(device)\n",
    "            )\n",
    "            doc_embeddings.append(d_emb.cpu().numpy())\n",
    "    \n",
    "    query_embeddings = np.vstack(query_embeddings)\n",
    "    doc_embeddings = np.vstack(doc_embeddings)\n",
    "    \n",
    "    # 计算相似度\n",
    "    print(\"  计算相似度矩阵...\")\n",
    "    similarities = cosine_similarity(query_embeddings, doc_embeddings)\n",
    "    \n",
    "    # 计算 Recall@K 和 MRR\n",
    "    print(f\"  计算 Recall@{k} 和 MRR...\")\n",
    "    recall_at_k = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        top_k_indices = np.argsort(similarities[i])[::-1][:k]\n",
    "        \n",
    "        if i in top_k_indices:\n",
    "            recall_at_k.append(1)\n",
    "            rank = np.where(top_k_indices == i)[0][0] + 1\n",
    "            mrr_scores.append(1.0 / rank)\n",
    "        else:\n",
    "            recall_at_k.append(0)\n",
    "            mrr_scores.append(0)\n",
    "    \n",
    "    results = {\n",
    "        f'Recall@{k}': np.mean(recall_at_k),\n",
    "        'MRR': np.mean(mrr_scores)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"评估函数定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ec8be-f97f-48d9-b86f-d3dc5a2fc9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "验证集评估\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 已加载字符级模型（CPU）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: 在验证集上评估两个模型\n",
    "print(\"=\"*60)\n",
    "print(\"验证集评估\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============ 确保必要组件已定义 ============\n",
    "try:\n",
    "    EmbeddingModel\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    print(\"重新定义 EmbeddingModel...\")\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    \n",
    "    MODEL_NAME = '/root/bert-base-chinese'\n",
    "    \n",
    "    class EmbeddingModel(nn.Module):\n",
    "        def __init__(self, model_name=MODEL_NAME):\n",
    "            super().__init__()\n",
    "            self.encoder = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        def mean_pooling(self, token_embeddings, attention_mask):\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "        \n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
    "            return F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(\"✓ 组件定义完成\\n\")\n",
    "\n",
    "# ============ 加载训练好的模型（强制 CPU）============\n",
    "char_model = EmbeddingModel(MODEL_NAME)\n",
    "char_model.load_state_dict(torch.load('字符级_epoch3.pt', map_location='cpu'))  # 添加 map_location='cpu'\n",
    "char_model.eval()  # 设置为评估模式\n",
    "print(\"✓ 已加载字符级模型（CPU）\")\n",
    "\n",
    "pinyin_model = EmbeddingModel(MODEL_NAME)\n",
    "pinyin_model.load_state_dict(torch.load('拼音级_epoch3.pt', map_location='cpu'))  # 添加 map_location='cpu'\n",
    "pinyin_model.eval()  # 设置为评估模式\n",
    "print(\"✓ 已加载拼音级模型（CPU）\\n\")\n",
    "\n",
    "# ============ 评估字符级模型 ============\n",
    "print(\"-\" * 60)\n",
    "print(\"评估字符级模型（验证集）...\")\n",
    "print(\"-\" * 60)\n",
    "val_q_test=val_q[:500]\n",
    "val_d_test=val_d[:500]\n",
    "char_val_results = evaluate_retrieval(\n",
    "    char_model,\n",
    "    val_q,\n",
    "    val_d,\n",
    "    tokenizer,\n",
    "    use_pinyin=False,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(\"\\n字符级模型（验证集）:\")\n",
    "for metric, value in char_val_results.items():\n",
    "    print(f\"  {metric:<15}: {value:.4f}\")\n",
    "\n",
    "# ============ 评估拼音级模型 ============\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"评估拼音级模型（验证集）...\")\n",
    "print(\"-\" * 60)\n",
    "pinyin_val_results = evaluate_retrieval(\n",
    "    pinyin_model,\n",
    "    val_q,\n",
    "    val_d,\n",
    "    tokenizer,\n",
    "    use_pinyin=True,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(\"\\n拼音级模型（验证集）:\")\n",
    "for metric, value in pinyin_val_results.items():\n",
    "    print(f\"  {metric:<15}: {value:.4f}\")\n",
    "\n",
    "# ============ 验证集对比 ============\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"验证集对比:\")\n",
    "print(\"=\"*60)\n",
    "for metric in char_val_results.keys():\n",
    "    char_val = char_val_results[metric]\n",
    "    pinyin_val = pinyin_val_results[metric]\n",
    "    diff = pinyin_val - char_val\n",
    "    diff_pct = (diff / char_val) * 100 if char_val > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{metric}\")\n",
    "    print(f\"  字符级: {char_val:.4f}\")\n",
    "    print(f\"  拼音级: {pinyin_val:.4f}\")\n",
    "    print(f\"  差异:   {diff:+.4f} ({diff_pct:+.1f}%)\")\n",
    "\n",
    "# 保存结果供可视化使用\n",
    "char_results = char_val_results\n",
    "pinyin_results = pinyin_val_results\n",
    "\n",
    "print(\"\\n✓ 验证集评估完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15621af6-933a-493c-8389-b3b9be24d7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "%cd autodl-tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4560d-4e0b-42cc-b0d8-3ba9b0388943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
